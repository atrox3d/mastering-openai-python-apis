{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import openai\n",
    "from dotenv import dotenv_values, load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m reply = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdavinci-002\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m reply\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/resources/completions.py:539\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    512\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    537\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    538\u001b[39m ) -> Completion | Stream[Completion]:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbest_of\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mecho\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msuffix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCompletion\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:1296\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1284\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1291\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1292\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1293\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1294\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1295\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:973\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    971\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:1062\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1061\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:1111\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1107\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m   1108\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m   1109\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:1062\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1061\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:1111\u001b[39m, in \u001b[36mSyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1107\u001b[39m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[32m   1108\u001b[39m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[32m   1109\u001b[39m time.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python/ai/openai/mastering-openai-python-apis/.venv/lib/python3.12/site-packages/openai/_base_client.py:1077\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1074\u001b[39m         err.response.read()\n\u001b[32m   1076\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1080\u001b[39m     cast_to=cast_to,\n\u001b[32m   1081\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1086\u001b[39m )\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "reply = client.completions.create(\n",
    "    prompt='test',\n",
    "    model=\"davinci-002\",\n",
    "    \n",
    ")\n",
    "\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-B4SlOiIlRy8pPZqXlj4a9ez28NOi2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The cat says \"meow\"! Cats use this sound to communicate with humans and other cats. Is there something specific you\\'d like to know about cats or their behavior?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1740403578, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_7fcd609668', usage=CompletionUsage(completion_tokens=34, prompt_tokens=11, total_tokens=45, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"the cat says \",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # model=\"davinci-002\",\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-B4SlOiIlRy8pPZqXlj4a9ez28NOi2\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The cat says \\\"meow\\\"! Cats use this sound to communicate with humans and other cats. Is there something specific you'd like to know about cats or their behavior?\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1740403578,\n",
      "    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": \"default\",\n",
      "    \"system_fingerprint\": \"fp_7fcd609668\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 34,\n",
      "        \"prompt_tokens\": 11,\n",
      "        \"total_tokens\": 45,\n",
      "        \"completion_tokens_details\": {\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"audio_tokens\": 0,\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        },\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"audio_tokens\": 0,\n",
      "            \"cached_tokens\": 0\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(response.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"completion_tokens\": 34,\n",
      "    \"prompt_tokens\": 11,\n",
      "    \"total_tokens\": 45,\n",
      "    \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(response.usage.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.Choice"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-B4TBRxq3IBv6UfrM7S7oBfnYr4kKN',\n",
       " 'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': {'content': [{'token': 'As',\n",
       "      'bytes': [65, 115],\n",
       "      'logprob': -0.0011911796,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' of',\n",
       "      'bytes': [32, 111, 102],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' my',\n",
       "      'bytes': [32, 109, 121],\n",
       "      'logprob': -0.069115974,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' last',\n",
       "      'bytes': [32, 108, 97, 115, 116],\n",
       "      'logprob': -0.0007320932,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' update',\n",
       "      'bytes': [32, 117, 112, 100, 97, 116, 101],\n",
       "      'logprob': -0.7008254,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' in',\n",
       "      'bytes': [32, 105, 110],\n",
       "      'logprob': -0.57596207,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' October',\n",
       "      'bytes': [32, 79, 99, 116, 111, 98, 101, 114],\n",
       "      'logprob': -0.0010567454,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' ', 'bytes': [32], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '202',\n",
       "      'bytes': [50, 48, 50],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '3', 'bytes': [51], 'logprob': -0.3132657, 'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -5.5122365e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'logprob': -0.018212223,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' following',\n",
       "      'bytes': [32, 102, 111, 108, 108, 111, 119, 105, 110, 103],\n",
       "      'logprob': -1.993747,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' are',\n",
       "      'bytes': [32, 97, 114, 101],\n",
       "      'logprob': -0.22003628,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' often',\n",
       "      'bytes': [32, 111, 102, 116, 101, 110],\n",
       "      'logprob': -1.5646949,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cited',\n",
       "      'bytes': [32, 99, 105, 116, 101, 100],\n",
       "      'logprob': -0.33703604,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'bytes': [32, 97, 115],\n",
       "      'logprob': -0.03804333,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'logprob': -0.079021096,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' most',\n",
       "      'bytes': [32, 109, 111, 115, 116],\n",
       "      'logprob': -0.48470572,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' populated',\n",
       "      'bytes': [32, 112, 111, 112, 117, 108, 97, 116, 101, 100],\n",
       "      'logprob': -0.06202538,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' cities',\n",
       "      'bytes': [32, 99, 105, 116, 105, 101, 115],\n",
       "      'logprob': -0.00013727049,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' in',\n",
       "      'bytes': [32, 105, 110],\n",
       "      'logprob': -0.004045037,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'logprob': -0.00043078844,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' world',\n",
       "      'bytes': [32, 119, 111, 114, 108, 100],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': -0.28991854, 'top_logprobs': []},\n",
       "     {'token': ' based',\n",
       "      'bytes': [32, 98, 97, 115, 101, 100],\n",
       "      'logprob': -0.53405833,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' on',\n",
       "      'bytes': [32, 111, 110],\n",
       "      'logprob': -0.0045950804,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' city',\n",
       "      'bytes': [32, 99, 105, 116, 121],\n",
       "      'logprob': -2.1269655,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' proper',\n",
       "      'bytes': [32, 112, 114, 111, 112, 101, 114],\n",
       "      'logprob': -0.00063702365,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' populations',\n",
       "      'bytes': [32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110, 115],\n",
       "      'logprob': -1.6119566,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' (',\n",
       "      'bytes': [32, 40],\n",
       "      'logprob': -1.1617346,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'not',\n",
       "      'bytes': [110, 111, 116],\n",
       "      'logprob': -0.1932632,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' metropolitan',\n",
       "      'bytes': [32, 109, 101, 116, 114, 111, 112, 111, 108, 105, 116, 97, 110],\n",
       "      'logprob': -0.055848774,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' areas',\n",
       "      'bytes': [32, 97, 114, 101, 97, 115],\n",
       "      'logprob': -0.25898895,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '):\\n\\n',\n",
       "      'bytes': [41, 58, 10, 10],\n",
       "      'logprob': -0.4406038,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '1',\n",
       "      'bytes': [49],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': -0.008618828,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Tokyo',\n",
       "      'bytes': [84, 111, 107, 121, 111],\n",
       "      'logprob': -9.2457245e-05,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -0.0031882864,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Japan',\n",
       "      'bytes': [32, 74, 97, 112, 97, 110],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -0.8259878,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '2',\n",
       "      'bytes': [50],\n",
       "      'logprob': -1.9816675e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Delhi',\n",
       "      'bytes': [68, 101, 108, 104, 105],\n",
       "      'logprob': -0.0015516406,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' India',\n",
       "      'bytes': [32, 73, 110, 100, 105, 97],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -5.5577775e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '3', 'bytes': [51], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Shanghai',\n",
       "      'bytes': [83, 104, 97, 110, 103, 104, 97, 105],\n",
       "      'logprob': -0.0004294772,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' China',\n",
       "      'bytes': [32, 67, 104, 105, 110, 97],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -3.7697225e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '4', 'bytes': [52], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'São',\n",
       "      'bytes': [83, 195, 163, 111],\n",
       "      'logprob': -0.025022784,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Paulo',\n",
       "      'bytes': [32, 80, 97, 117, 108, 111],\n",
       "      'logprob': -3.1281633e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' Brazil',\n",
       "      'bytes': [32, 66, 114, 97, 122, 105, 108],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -2.9352968e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '5', 'bytes': [53], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Mexico',\n",
       "      'bytes': [77, 101, 120, 105, 99, 111],\n",
       "      'logprob': -0.01182318,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' City',\n",
       "      'bytes': [32, 67, 105, 116, 121],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' Mexico',\n",
       "      'bytes': [32, 77, 101, 120, 105, 99, 111],\n",
       "      'logprob': -3.1281633e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -3.7697225e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '6', 'bytes': [54], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'C',\n",
       "      'bytes': [67],\n",
       "      'logprob': -0.026452636,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'airo',\n",
       "      'bytes': [97, 105, 114, 111],\n",
       "      'logprob': -7.465036e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Egypt',\n",
       "      'bytes': [32, 69, 103, 121, 112, 116],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -1.6240566e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '7',\n",
       "      'bytes': [55],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Mumbai',\n",
       "      'bytes': [77, 117, 109, 98, 97, 105],\n",
       "      'logprob': -1.1375201,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -0.006715371,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' India',\n",
       "      'bytes': [32, 73, 110, 100, 105, 97],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -2.0458236e-05,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '8', 'bytes': [56], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Be',\n",
       "      'bytes': [66, 101],\n",
       "      'logprob': -0.0003712086,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ijing',\n",
       "      'bytes': [105, 106, 105, 110, 103],\n",
       "      'logprob': -4.365741e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' China',\n",
       "      'bytes': [32, 67, 104, 105, 110, 97],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -4.246537e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '9',\n",
       "      'bytes': [57],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Dh',\n",
       "      'bytes': [68, 104],\n",
       "      'logprob': -6.945094e-05,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'aka',\n",
       "      'bytes': [97, 107, 97],\n",
       "      'logprob': -9.729906e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' Bangladesh',\n",
       "      'bytes': [32, 66, 97, 110, 103, 108, 97, 100, 101, 115, 104],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n',\n",
       "      'bytes': [42, 42, 10],\n",
       "      'logprob': -5.5577775e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '10', 'bytes': [49, 48], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': 0.0, 'top_logprobs': []},\n",
       "     {'token': ' **',\n",
       "      'bytes': [32, 42, 42],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Os',\n",
       "      'bytes': [79, 115],\n",
       "      'logprob': -0.00014740281,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'aka',\n",
       "      'bytes': [97, 107, 97],\n",
       "      'logprob': -0.00017004783,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -1.2664457e-06,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' Japan',\n",
       "      'bytes': [32, 74, 97, 112, 97, 110],\n",
       "      'logprob': 0.0,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '**\\n\\n',\n",
       "      'bytes': [42, 42, 10, 10],\n",
       "      'logprob': -0.0003011425,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'Please',\n",
       "      'bytes': [80, 108, 101, 97, 115, 101],\n",
       "      'logprob': -0.23254125,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' note',\n",
       "      'bytes': [32, 110, 111, 116, 101],\n",
       "      'logprob': -0.022627292,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' that',\n",
       "      'bytes': [32, 116, 104, 97, 116],\n",
       "      'logprob': -0.00020914296,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' population',\n",
       "      'bytes': [32, 112, 111, 112, 117, 108, 97, 116, 105, 111, 110],\n",
       "      'logprob': -0.11584452,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' figures',\n",
       "      'bytes': [32, 102, 105, 103, 117, 114, 101, 115],\n",
       "      'logprob': -0.10173414,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' are',\n",
       "      'bytes': [32, 97, 114, 101],\n",
       "      'logprob': -2.0483515,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' constantly',\n",
       "      'bytes': [32, 99, 111, 110, 115, 116, 97, 110, 116, 108, 121],\n",
       "      'logprob': -0.18174542,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' changing',\n",
       "      'bytes': [32, 99, 104, 97, 110, 103, 105, 110, 103],\n",
       "      'logprob': -0.00011176824,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' due',\n",
       "      'bytes': [32, 100, 117, 101],\n",
       "      'logprob': -0.15351759,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'bytes': [32, 116, 111],\n",
       "      'logprob': -6.704273e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' factors',\n",
       "      'bytes': [32, 102, 97, 99, 116, 111, 114, 115],\n",
       "      'logprob': -0.98815906,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' such',\n",
       "      'bytes': [32, 115, 117, 99, 104],\n",
       "      'logprob': -1.1369032,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' as',\n",
       "      'bytes': [32, 97, 115],\n",
       "      'logprob': -1.9361265e-07,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' urban',\n",
       "      'bytes': [32, 117, 114, 98, 97, 110],\n",
       "      'logprob': -0.13398007,\n",
       "      'top_logprobs': []},\n",
       "     {'token': 'ization',\n",
       "      'bytes': [105, 122, 97, 116, 105, 111, 110],\n",
       "      'logprob': -0.013765354,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',', 'bytes': [44], 'logprob': -0.11284531, 'top_logprobs': []},\n",
       "     {'token': ' migration',\n",
       "      'bytes': [32, 109, 105, 103, 114, 97, 116, 105, 111, 110],\n",
       "      'logprob': -0.029948566,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -0.00016301482,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' and',\n",
       "      'bytes': [32, 97, 110, 100],\n",
       "      'logprob': -0.00066716387,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' demographic',\n",
       "      'bytes': [32, 100, 101, 109, 111, 103, 114, 97, 112, 104, 105, 99],\n",
       "      'logprob': -0.5498751,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' shifts',\n",
       "      'bytes': [32, 115, 104, 105, 102, 116, 115],\n",
       "      'logprob': -1.6510632,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.', 'bytes': [46], 'logprob': -0.47436234, 'top_logprobs': []},\n",
       "     {'token': ' For',\n",
       "      'bytes': [32, 70, 111, 114],\n",
       "      'logprob': -0.4481125,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' the',\n",
       "      'bytes': [32, 116, 104, 101],\n",
       "      'logprob': -0.001832492,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' most',\n",
       "      'bytes': [32, 109, 111, 115, 116],\n",
       "      'logprob': -0.062356405,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' current',\n",
       "      'bytes': [32, 99, 117, 114, 114, 101, 110, 116],\n",
       "      'logprob': -0.6383023,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' statistics',\n",
       "      'bytes': [32, 115, 116, 97, 116, 105, 115, 116, 105, 99, 115],\n",
       "      'logprob': -0.9229397,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ',',\n",
       "      'bytes': [44],\n",
       "      'logprob': -0.0040192762,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' refer',\n",
       "      'bytes': [32, 114, 101, 102, 101, 114],\n",
       "      'logprob': -3.814837,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' to',\n",
       "      'bytes': [32, 116, 111],\n",
       "      'logprob': -1.3186812e-05,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' reliable',\n",
       "      'bytes': [32, 114, 101, 108, 105, 97, 98, 108, 101],\n",
       "      'logprob': -2.0300643,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' demographic',\n",
       "      'bytes': [32, 100, 101, 109, 111, 103, 114, 97, 112, 104, 105, 99],\n",
       "      'logprob': -0.46689177,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' sources',\n",
       "      'bytes': [32, 115, 111, 117, 114, 99, 101, 115],\n",
       "      'logprob': -0.6755841,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' or',\n",
       "      'bytes': [32, 111, 114],\n",
       "      'logprob': -0.050590303,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' governmental',\n",
       "      'bytes': [32, 103, 111, 118, 101, 114, 110, 109, 101, 110, 116, 97, 108],\n",
       "      'logprob': -4.284902,\n",
       "      'top_logprobs': []},\n",
       "     {'token': ' data',\n",
       "      'bytes': [32, 100, 97, 116, 97],\n",
       "      'logprob': -1.7696519,\n",
       "      'top_logprobs': []},\n",
       "     {'token': '.',\n",
       "      'bytes': [46],\n",
       "      'logprob': -0.0065073017,\n",
       "      'top_logprobs': []}],\n",
       "    'refusal': None},\n",
       "   'message': {'content': 'As of my last update in October 2023, the following are often cited as the most populated cities in the world, based on city proper populations (not metropolitan areas):\\n\\n1. **Tokyo, Japan**\\n2. **Delhi, India**\\n3. **Shanghai, China**\\n4. **São Paulo, Brazil**\\n5. **Mexico City, Mexico**\\n6. **Cairo, Egypt**\\n7. **Mumbai, India**\\n8. **Beijing, China**\\n9. **Dhaka, Bangladesh**\\n10. **Osaka, Japan**\\n\\nPlease note that population figures are constantly changing due to factors such as urbanization, migration, and demographic shifts. For the most current statistics, refer to reliable demographic sources or governmental data.',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': None}}],\n",
       " 'created': 1740405193,\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': 'default',\n",
       " 'system_fingerprint': 'fp_709714d124',\n",
       " 'usage': {'completion_tokens': 149,\n",
       "  'prompt_tokens': 17,\n",
       "  'total_tokens': 166,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"the top 10 most populated cities are: \",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # model=\"davinci-002\",\n",
    "    logprobs=True\n",
    ")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"tell me a unique joke about death\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o-mini\",\n",
    "    n=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the ghost break up with the skeleton?\\n\\nBecause he didn’t have the guts to commit!',\n",
       " 'Why did the skeleton go to the party alone? \\n\\nBecause he had no body to go with him!',\n",
       " 'Why don\\'t skeletons fight each other? \\n\\nThey don\\'t have the guts, but they sure know how to \"bone\" up on their last words!',\n",
       " \"Why don't skeletons fight each other?\\n\\nThey don't have the guts!\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[choice.message.content for choice in response.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the ghost break up with the skeleton?\\n\\nBecause he didn’t have the guts to commit!',\n",
       " 'Why did the skeleton go to the party alone? \\n\\nBecause he had no body to go with him!',\n",
       " 'Why don\\'t skeletons fight each other? \\n\\nThey don\\'t have the guts, but they sure know how to \"bone\" up on their last words!',\n",
       " \"Why don't skeletons fight each other?\\n\\nThey don't have the guts!\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[choice['message']['content'] for choice in response.model_dump()['choices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [choice.message.content for choice in response.choices] == [choice['message']['content'] for choice in response.model_dump()['choices']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
